{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)\tGoal: observe the performance of classification on difficult vs easy datasets\n",
    "#### Newsgroup data has 6 main classes with multiple subclasses for each class. The main classes include Computer, recreation, science, politics, miscellaneous and other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a.\tPerform Naiive bayes classification on an easy data set that includes the classes (recreation, computers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sys module allows python to interface with underlying operating system and access file system without worrying about underlying operating system. \n",
    "import sys \n",
    "# os module is same as above but contains ability to rename\n",
    "import os\n",
    "# NumPy (Numeric Python) has powerful data structures and function for efficient computation of multi-dimen\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "news = fetch_20newsgroups()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['description', 'DESCR', 'filenames', 'target_names', 'data', 'target']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "news.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism',\n",
      " 'comp.graphics',\n",
      " 'comp.os.ms-windows.misc',\n",
      " 'comp.sys.ibm.pc.hardware',\n",
      " 'comp.sys.mac.hardware',\n",
      " 'comp.windows.x',\n",
      " 'misc.forsale',\n",
      " 'rec.autos',\n",
      " 'rec.motorcycles',\n",
      " 'rec.sport.baseball',\n",
      " 'rec.sport.hockey',\n",
      " 'sci.crypt',\n",
      " 'sci.electronics',\n",
      " 'sci.med',\n",
      " 'sci.space',\n",
      " 'soc.religion.christian',\n",
      " 'talk.politics.guns',\n",
      " 'talk.politics.mideast',\n",
      " 'talk.politics.misc',\n",
      " 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "# printing list of categories\n",
    "pprint(list(news.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['comp.sys.ibm.pc.hardware', 'rec.sport.baseball']\n"
     ]
    }
   ],
   "source": [
    "news = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    categories=('comp.sys.ibm.pc.hardware','rec.sport.baseball'), \n",
    "    remove=('headers','footers','quotes'))\n",
    "pprint(list(news.target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has 500 rows and 7430 columns\n"
     ]
    }
   ],
   "source": [
    "#generate term frequency matrix\n",
    "#feature_extraction.text is a module that turns text into vectors of numerical values suitable for statistical analysis. \n",
    "#CountVectorizer converts a collection of text documents into a matrix of token counts\n",
    "#We will create below counts for the 10,000 most frequent single word tokens ignoring those words with a document frequency > 500 and english stop words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "tf_vec = CountVectorizer (max_df=500,\n",
    "                         min_df=0,\n",
    "                         max_features=10000,\n",
    "                         ngram_range=(1,1),\n",
    "                         stop_words='english')\n",
    "#this creates a sparse matrix in which only non-zero values are stored \n",
    "#(each row is document/token pair w/ non-zero count, then count)\n",
    "tf_matrix = tf_vec.fit_transform(news.data[:500])\n",
    "print ('the data has %d rows and %d columns' % (tf_matrix.shape[0], tf_matrix.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1242)\t1\n",
      "  (0, 2843)\t1\n",
      "  (0, 4883)\t1\n",
      "  (0, 6809)\t1\n",
      "  (0, 6356)\t1\n",
      "  (0, 2204)\t1\n",
      "  (0, 5597)\t2\n",
      "  (0, 7191)\t1\n",
      "  (0, 2138)\t1\n",
      "  (0, 6094)\t1\n",
      "  (0, 4540)\t1\n",
      "  (0, 3888)\t1\n",
      "  (0, 4577)\t2\n",
      "  (0, 6745)\t1\n",
      "  (0, 2174)\t1\n",
      "  (0, 386)\t1\n",
      "  (0, 247)\t1\n",
      "  (0, 358)\t1\n",
      "  (0, 1328)\t1\n",
      "  (0, 4411)\t1\n",
      "  (0, 3090)\t2\n",
      "  (0, 261)\t2\n",
      "  (0, 2920)\t3\n",
      "  (0, 6933)\t2\n",
      "  (0, 2099)\t1\n",
      "  :\t:\n",
      "  (8, 4292)\t2\n",
      "  (8, 6554)\t1\n",
      "  (8, 6192)\t1\n",
      "  (8, 5416)\t1\n",
      "  (8, 3378)\t1\n",
      "  (8, 7192)\t1\n",
      "  (8, 1510)\t1\n",
      "  (8, 5609)\t1\n",
      "  (8, 6381)\t1\n",
      "  (8, 4843)\t1\n",
      "  (8, 7023)\t1\n",
      "  (8, 3076)\t2\n",
      "  (8, 7251)\t1\n",
      "  (8, 6010)\t1\n",
      "  (8, 2924)\t2\n",
      "  (8, 7343)\t1\n",
      "  (8, 6260)\t1\n",
      "  (8, 5533)\t1\n",
      "  (8, 2454)\t1\n",
      "  (8, 5813)\t1\n",
      "  (8, 7037)\t1\n",
      "  (8, 7191)\t1\n",
      "  (8, 261)\t1\n",
      "  (8, 4260)\t1\n",
      "  (8, 5944)\t1\n"
     ]
    }
   ],
   "source": [
    "print (tf_matrix[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   00  000  007  01  02  03  0300  031  0334  038   ...    zbiciak  zeile  \\\n",
      "1   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "2   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "3   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "4   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "5   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "6   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "7   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "8   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "9   0    0    0   0   0   0     0    0     0    0   ...          0      0   \n",
      "\n",
      "   zenith  zeos  zero  zip  zone  zoom  zorro  zupcic  \n",
      "1       0     0     0    0     0     0      0       0  \n",
      "2       0     0     0    0     0     0      0       0  \n",
      "3       0     0     0    0     0     0      0       0  \n",
      "4       0     0     0    0     0     0      0       0  \n",
      "5       0     0     0    0     0     0      0       0  \n",
      "6       0     0     0    0     0     0      0       0  \n",
      "7       0     0     0    0     0     0      0       0  \n",
      "8       0     0     0    0     0     0      0       0  \n",
      "9       0     0     0    0     0     0      0       0  \n",
      "\n",
      "[9 rows x 7430 columns]\n"
     ]
    }
   ],
   "source": [
    "#pandas module is library for data manipulation and analysis, especially to manipulate time series and numerical tables\n",
    "import pandas as pd\n",
    "#DataFrame function creates a spreadsheet essentially in which zero values are also stored\n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=tf_vec.get_feature_names())\n",
    "print (full_matrix[1:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "#classify data\n",
    "#we will make an array of the true category value for each post\n",
    "t=np.asarray(news.target[:500])\n",
    "print t [1:10]\n",
    "#train_test_split module splits our full matrix array and our category array (formatted as numpy array) \n",
    "#into random train and test subsets\n",
    "from sklearn.model_selection import train_test_split\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of mislabels out of 125 points: 6\n"
     ]
    }
   ],
   "source": [
    "# naive_bayes module assuming normal distribution of features (Gaussian)\n",
    "# popular baseline method for text categorization using word frequencies as features\n",
    "# meaning: for each word(feature) in the document, \n",
    "# the training set has calculated a prior probability of that word(feature) belonging to a particular category \n",
    "from sklearn.naive_bayes import GaussianNB as NB\n",
    "clf = NB()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.\tPerform Naiive Bayes classification on a difficult dataset that includes the classes rec.motorcycles and rec.autos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has 500 rows and 7430 columns\n",
      "number of mislabels out of 125 points: 60\n"
     ]
    }
   ],
   "source": [
    "news2 = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    categories=('rec.motorcycles','rec.autos'), \n",
    "    remove=('headers','footers','quotes'))\n",
    "tf_vec2 = CountVectorizer (max_df=500,\n",
    "                         min_df=0,\n",
    "                         max_features=10000,\n",
    "                         ngram_range=(1,1),\n",
    "                         stop_words='english')\n",
    "tf_matrix2 = tf_vec2.fit_transform(news2.data[:500])\n",
    "print ('the data has %d rows and %d columns' % (tf_matrix.shape[0], tf_matrix.shape[1]))\n",
    "\n",
    "full_matrix2 = pd.DataFrame(tf_matrix2.todense(),columns=tf_vec2.get_feature_names())\n",
    "t2=np.asarray(news.target[:500])\n",
    "xtrain2, xtest2, ytrain2, ytest2 = train_test_split(full_matrix2.as_matrix(),t2,random_state=50) \n",
    "y_pred2 = clf.fit(xtrain2, ytrain2).predict(xtest2)\n",
    "error = (y_pred2 != ytest2).sum()\n",
    "print (\"number of mislabels out of %d points: %d\" % (xtest2.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c.\tRepeat a and b using the decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the easy data set, number of mislabels out of 125 points: 18\n",
      "for the difficult data set, number of mislabels out of 125 points: 64\n"
     ]
    }
   ],
   "source": [
    "# tree module includes decision tree-based models for classification and regression\n",
    "from sklearn import tree\n",
    "# Decision Tree Classifier generates an algorithm based on feature values that classifies the data\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "y_pred = clf.fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"for the easy data set, number of mislabels out of %d points: %d\" % (xtest.shape[0],error ))\n",
    "y_pred2 = clf.fit(xtrain2, ytrain2).predict(xtest2)\n",
    "error = (y_pred2 != ytest2).sum()\n",
    "print (\"for the difficult data set, number of mislabels out of %d points: %d\" % (xtest2.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d.\tDiscuss the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is popular for test classification and this exercise demonstrates why. Because there are so many words(features) in the text volume overall, Naive Bayes allows you to assess all relevant features in a document at one time and combine their prior probabilities to predict the class of the new document. Decision tree is less flexible. The algorithm is developed to pick the most polarized features first and then assign class based on value of one feature at a time. Once classified, the algorithm can't then reassess the other features in the document to refine the likelihood of the correct class. So it makes sense that the decision tree classifiers perform less well than the naive bayes classifiers.  But naive bayes seems to lose it's power as the classification becomes more subtle (as when the text documents are on very similar topics) and error rate approaches that of decision tree. Which by the way is pretty close to chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat the classification of the difficult data set above using:\n",
    "\n",
    "a.\tBagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Bagging - number of mislabels out of 125 points: 65\n"
     ]
    }
   ],
   "source": [
    "# Bagging or Bootstrap Aggregation takes a sample data set with replacement to train and generates a classifier\n",
    "# Classification in this case is naive bayes\n",
    "# This process is repeated over and over.  Then all classifiers are applied a novel data set.\n",
    "# For each target, the class with the most votes wins!\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "# in this case we are using base estimator of naive bayes, a default number of 10 times\n",
    "# not sure what max_samples and max_features means\n",
    "bagging = BaggingClassifier (NB(), max_samples=.5, max_features=.5)\n",
    "y_pred2 = bagging.fit(xtrain2, ytrain2).predict(xtest2)\n",
    "error = (y_pred2 != ytest2).sum()\n",
    "print (\"With Bagging - number of mislabels out of %d points: %d\" % (xtest2.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b.\tAdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With AdaBoost - number of mislabels out of 125 points: 60\n"
     ]
    }
   ],
   "source": [
    "# AdaBoost generates a classifier, then gives more weight to the points that were incorrectly classified.\n",
    "# Then generates another classifier, and so on\n",
    "# Each classifier is weighted based on its accuracy and the votes are combined\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# Here we again using base estimator of naive bayes 300 times using the SAMME algorithm\n",
    "# Discrete SAMME AdaBoost adapts based on errors in predicted class labels whereas real SAMME.R uses the predicted class probabilities.\n",
    "clf = AdaBoostClassifier(NB(),\n",
    "                         algorithm=\"SAMME\",\n",
    "                         n_estimators=300)\n",
    "\n",
    "y_pred2 = clf.fit(xtrain2, ytrain2).predict(xtest2)\n",
    "error = (y_pred2 != ytest2).sum()\n",
    "print (\"With AdaBoost - number of mislabels out of %d points: %d\" % (xtest2.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c.\tRandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With Random Forest- number of mislabels out of 125 points: 61\n"
     ]
    }
   ],
   "source": [
    "# RandomForst generates a classifier (tree) using a bootstrap sample, for each node a set of random features are chosen\n",
    "# the best feature to divide the sample is chosen to split the data \n",
    "# a bunch of additional classifiers are generated in the same way and then votes are combined\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# here we are generating 100 trees with each node assessing sqrt of our 10,000 features\n",
    "clf = RandomForestClassifier(n_estimators=100, max_depth=None,random_state=10, max_features='auto')\n",
    "y_pred2 = clf.fit(xtrain2, ytrain2).predict(xtest2)\n",
    "error = (y_pred2 != ytest2).sum()\n",
    "print (\"With Random Forest- number of mislabels out of %d points: %d\" % (xtest2.shape[0],error ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare performance of the ensemble classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each time they are run, they give a slightly different answer all hovering around 60. There seems to be more variation in Bagging's output than AdaBoost or Randome Forest but mostly, they perform about equally (certain within the margin of error).  But none of them help me to better categorize documents as auto or motorcycle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment with (10,20,30)-fold cross validation and discuss whether increasing the folds affects the stability of the ensembles performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Accuracy of fold (1) is 0.176\n",
      " Accuracy of fold (2) is 0.353\n",
      " Accuracy of fold (3) is 0.471\n",
      " Accuracy of fold (4) is 0.471\n",
      " Accuracy of fold (5) is 0.529\n",
      " Accuracy of fold (6) is 0.529\n",
      " Accuracy of fold (7) is 0.529\n",
      " Accuracy of fold (8) is 0.294\n",
      " Accuracy of fold (9) is 0.824\n",
      " Accuracy of fold (10) is 0.353\n",
      " Accuracy of fold (11) is 0.765\n",
      " Accuracy of fold (12) is 0.471\n",
      " Accuracy of fold (13) is 0.529\n",
      " Accuracy of fold (14) is 0.647\n",
      " Accuracy of fold (15) is 0.294\n",
      " Accuracy of fold (16) is 0.706\n",
      " Accuracy of fold (17) is 0.294\n",
      " Accuracy of fold (18) is 0.353\n",
      " Accuracy of fold (19) is 0.588\n",
      " Accuracy of fold (20) is 0.529\n",
      " Accuracy of fold (21) is 0.375\n",
      " Accuracy of fold (22) is 0.625\n",
      " Accuracy of fold (23) is 0.688\n",
      " Accuracy of fold (24) is 0.375\n",
      " Accuracy of fold (25) is 0.438\n",
      " Accuracy of fold (26) is 0.688\n",
      " Accuracy of fold (27) is 0.312\n",
      " Accuracy of fold (28) is 0.688\n",
      " Accuracy of fold (29) is 0.562\n",
      " Accuracy of fold (30) is 0.438\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "t=np.asarray(news2.target[:500])   # true labels\n",
    "kf = KFold(n_splits =30)\n",
    "i=0\n",
    "for train, test in kf.split(tf_matrix): \n",
    "    xtrain,xtest = tf_matrix[train],  tf_matrix[test]\n",
    "    ytrain, ytest = t[train], t[test]\n",
    "    #clf = RandomForestClassifier(n_estimators=100, max_depth=None,random_state=10, max_features='auto')\n",
    "    #clf = NB()\n",
    "    clf = AdaBoostClassifier(NB(),algorithm=\"SAMME\",n_estimators=300)\n",
    "    y = clf.fit(xtrain.toarray(), ytrain).predict(xtest.toarray())\n",
    "    acc=metrics.accuracy_score(ytest, y)\n",
    "    i=i+1\n",
    "\n",
    "    print (\" Accuracy of fold (%d) is %3.3f\" % (i,acc ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the folds does affect the stability of the ensemble classifiers performance negatively. The more folds you have, the wider the variation in accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)\t Multi-label classification:\n",
    "#### Form a sample of the following classes: comp.graphics, rec.autos, talk.politics.gun, soc.religion.christian. Include at least 200 documents from each subclass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the data has 2323 rows and 29847 columns\n"
     ]
    }
   ],
   "source": [
    "news = fetch_20newsgroups(\n",
    "    subset='train', \n",
    "    categories=('comp.graphics', 'rec.autos', 'talk.politics.guns', 'soc.religion.christian'), \n",
    "    remove=('headers','footers','quotes'))\n",
    "tf_vec = CountVectorizer (max_df=500,\n",
    "                         min_df=0,\n",
    "                         max_features=30000,\n",
    "                         ngram_range=(1,1),\n",
    "                         stop_words='english')\n",
    "tf_matrix = tf_vec.fit_transform(news.data)\n",
    "print ('the data has %d rows and %d columns' % (tf_matrix.shape[0], tf_matrix.shape[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform one-vs-all classification using NaÃ¯ve Bayes, decision trees and support vector machine SVC classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-All SVC --> number of mislabels out of 581 points in the test test: 92\n",
      "One-vs-All NB --> number of mislabels out of 581 points in the test test: 125\n"
     ]
    }
   ],
   "source": [
    "# one versus all classification develops classifiers by comparing one category to all others\n",
    "# then applies all developed classifiers to each new instance and the class with the highest confidence score wins\n",
    "full_matrix = pd.DataFrame(tf_matrix.todense(),columns=tf_vec.get_feature_names())\n",
    "t=np.asarray(news.target)\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(full_matrix.as_matrix(),t,random_state=50) \n",
    "\n",
    "# A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. \n",
    "# In other words, given labeled training data (supervised learning), the algorithm outputs an optimal hyperplane which categorizes new examples.\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf1= LinearSVC(random_state=10)\n",
    "clf2 = NB()\n",
    "clf3 = tree.DecisionTreeClassifier()\n",
    "\n",
    "for name, classifier in zip(['SVC','NB','DT'],[clf1,clf2,clf3]):\n",
    "    y_pred = OneVsRestClassifier(classifier).fit(xtrain, ytrain).predict(xtest)\n",
    "    error = (y_pred != ytest).sum()\n",
    "    print (\"One-vs-All %s --> number of mislabels out of %d points in the test test: %d\" % (name, xtest.shape[0],error ))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Repeat classification using all-vs-all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-vs-One --> SVC number of mislabels out of 581 points in the test test: 100\n",
      "accuracy  is 0.83 \n",
      "One-vs-One --> NB number of mislabels out of 581 points in the test test: 71\n",
      "accuracy  is 0.88 \n",
      "One-vs-One --> DT number of mislabels out of 581 points in the test test: 113\n",
      "accuracy  is 0.81 \n"
     ]
    }
   ],
   "source": [
    "# One-vs-One (All-vs-All)\n",
    "# one vs one makes a classifier for each pair of categories, then again runs all classifiers on new instance\n",
    "# which ever category has the most votes wins\n",
    "# \n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf1 = LinearSVC(random_state=10)\n",
    "clf2 = NB()\n",
    "clf3 = tree.DecisionTreeClassifier()\n",
    "\n",
    "y_pred = OneVsOneClassifier(clf1).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-One --> SVC number of mislabels out of %d points in the test test: %d\" % (xtest.shape[0],error ))\n",
    "print(\"accuracy  is %2.2f \" % (metrics.accuracy_score(ytest, y_pred)))\n",
    "y_pred = OneVsOneClassifier(clf2).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-One --> NB number of mislabels out of %d points in the test test: %d\" % (xtest.shape[0],error ))\n",
    "print(\"accuracy  is %2.2f \" % (metrics.accuracy_score(ytest, y_pred)))\n",
    "y_pred = OneVsOneClassifier(clf3).fit(xtrain, ytrain).predict(xtest)\n",
    "error = (y_pred != ytest).sum()\n",
    "print (\"One-vs-One --> DT number of mislabels out of %d points in the test test: %d\" % (xtest.shape[0],error ))\n",
    "print(\"accuracy  is %2.2f \" % (metrics.accuracy_score(ytest, y_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss the results for both approaches and rank classifiers based on performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this text categorization, naive bayes in one vs one performs the best. Categories are somewhat different making naive bayes a good choice but the one vs one is a better model than the one vs rest. For some reason one vs rest using decision tree keeps timing out but I did get it to run previous and it was similar to NB. \n",
    "1. ovo NB\n",
    "2. ovr SVC\n",
    "3. ovo SVC\n",
    "4. ovo DT\n",
    "5. ovo NB\n",
    "6. ovo DT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Assuming a decision stump classifier with the AdaBoost ensemble classifier. How does the classifier process the weights of the data points to focus on misclassified data points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each misclassified point is increased in weight so then a new stump that fits more of those points would be preferred over the original stump. I feel like I'm missing something here though.  Would love an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
